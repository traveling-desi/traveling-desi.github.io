<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="Traveling-desi.github.io : " />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Traveling-desi.github.io</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/traveling-desi">View on GitHub</a>

          <h1 id="project_title">Traveling-desi.github.io</h1>
          <h2 id="project_tagline"></h2>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>&lt;!DOCTYPE html&gt;</p>

<p></p>

<p></p>

<p></p>

<p></p>Practical Machine Learning Coursera class project<p></p>

code{white-space: pre;}<p></p>


  pre:not([class]) {
    background-color: white;
  }
<p></p>

<p></p>


.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
<div>


<div>
<h1>
<a name="practical-machine-learning-coursera-class-project" class="anchor" href="#practical-machine-learning-coursera-class-project"><span class="octicon octicon-link"></span></a>Practical Machine Learning Coursera class project</h1>
</div>

<div>
<h1>
<a name="introduction" class="anchor" href="#introduction"><span class="octicon octicon-link"></span></a>Introduction</h1>
<p>The goal of this assignment is to predict the exercises that subjets are doing based on the readings of various acclerometers in various belts attached to various parts of the body. We used data from website <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> which also gives explanation for the various features.</p>
<p>After downloading the testing and training data (already created for us) we do basic data pre-processing. The training data has ~20k records and 160 features while the testing data has 20 records and 160 features.</p>
<pre><code>## Load packages and load training and testing set
library(caret)</code></pre>
<pre><code>## Loading required package: lattice
## Loading required package: ggplot2</code></pre>
<pre><code>train &lt;- read.csv("pml-training.csv",header=TRUE,na.strings = c("NA",""))
test &lt;- read.csv("pml-testing.csv",header=TRUE,na.strings = c("NA",""))


## we have 19622 records and 159 features + one predicted variable
dim(train)</code></pre>
<pre><code>## [1] 19622   160</code></pre>
<pre><code>## we have 20 records and 159 features + column for problem_id
dim(test)</code></pre>
<pre><code>## [1]  20 160</code></pre>
</div>

<div>
<h1>
<a name="data-cleanup" class="anchor" href="#data-cleanup"><span class="octicon octicon-link"></span></a>Data Cleanup</h1>
<p>We notice that a large number of columns in the training set have NA (no value) in most of the records. Some columns have more than 97% of their record values as NA. These columns cannot meanningfully be expected to act as valid predictors. We remove all sunch colums. Also there are some columns which are information only and cannot be used as predictors. As example some of these columns are record #, user name, Time and data of observation etc. These values are for record keeping but not useful for prediction. The prediction variable (classe) is a factor variable that indicates what kind of exercise is being done (on fator leel A-E)</p>
<p>After performing such data cleanup we still have ~20k records but only 52 features.</p>
<pre><code>## DATA CLEANUP AND PREPROCESSING : Training set
## A lot of columns have majority NA in them. In our case most have more than 97% NA.
## We only keep/train on colummns that have less than 95% NA.
perc_of_na &lt;- function(d) {sum(is.na(d))/length(d)}
b &lt;- apply(train,2,perc_of_na)
clean_train &lt;- train[,b &lt; 0.95]

## Remove information varibales like user name, time/date of observation etc. since these 
## shouldnt matter to the class of actiivty being performed. Remove first seven columns
clean_train &lt;- clean_train[,-c(1:7)]

## We have 19622 records and  52 features + one predicted variable
dim(clean_train)</code></pre>
<pre><code>## [1] 19622    53</code></pre>
</div>

<div>
<h1>
<a name="cross-validation" class="anchor" href="#cross-validation"><span class="octicon octicon-link"></span></a>Cross Validation</h1>
<p>Next in order to find out out of sample error for the model we will train we set aside 25% of the trainig set as cross validation set(CV set.) The CV set is randomly sampled from the training set.</p>
<pre><code>## CROSS VALIDATION
## create parition of cleaned training data into training set and Cross Validation set to find
## out of sample error/accuracy.
clean_train_split &lt;- createDataPartition(y=clean_train$classe,p=0.75,list=FALSE)
sample_train &lt;- clean_train[clean_train_split,]
sample_cv &lt;- clean_train[-clean_train_split,]

## Set aside 25% of training records for cross valiation and find out the out of sample error.
dim(sample_train)</code></pre>
<pre><code>## [1] 14718    53</code></pre>
<pre><code>dim(sample_cv)</code></pre>
<pre><code>## [1] 4904   53</code></pre>
</div>

<div>
<h1>
<a name="model-fitting" class="anchor" href="#model-fitting"><span class="octicon octicon-link"></span></a>Model Fitting</h1>
<p>To train the model we use randomw forest from the caret package. Why did we use random forest? Because random forest is widely known to be extremely accurate and very easy to use (the interpertation of the model itself is harder though). As part of the training itself we do 4 fold crossvalidation (not to be confused with CV set aside earlier to measure the out of sample error)</p>
<pre><code>## MODEL TRAINING: RANDOM FOREST
modFit &lt;- train(classe ~ ., method="rf", data=sample_train, trControl = trainControl(method = "cv", number = 4))</code></pre>
<pre><code>## Loading required package: randomForest
## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<p>A quick check on the fitted model shows that out of the 52 variables available, if only 27 variables are randomly sampled we get the maximum accuracy. THis is automatically selected by the model.</p>
<pre><code>modFit</code></pre>
<pre><code>## Random Forest 
## 
## 14718 samples
##    52 predictors
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (4 fold) 
## 
## Summary of sample sizes: 11039, 11039, 11039, 11037 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
##   2     1         1      0.002        0.003   
##   30    1         1      6e-04        8e-04   
##   50    1         1      8e-04        0.001   
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 27.</code></pre>
<p>we can see the accuracy plot of the model against the # of variables randomly selected here:</p>
<p><img title="plot of chunk unnamed-chunk-8" alt="plot of chunk unnamed-chunk-8" width="672"></p>
<p>To predict the out of sample error for this model, we use this model on the CV set that we had set aside earlier. The Accuracy rate if about 99.29% which means out of sample erorr is about 0.71%. This is extremely good. We can summarize that the model was not overfit to the training data (low variance) and works reasonable well on data it has not seens before.</p>
<pre><code>## predict on the cross validation set to find the out of sample error/accuracy.
sample_predict &lt;- predict(modFit,sample_cv)
confusionMatrix(sample_predict,sample_cv$classe)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1394   11    0    0    0
##          B    1  936    4    0    0
##          C    0    1  848   14    0
##          D    0    1    3  790    0
##          E    0    0    0    0  901
## 
## Overall Statistics
##                                        
##                Accuracy : 0.993        
##                  95% CI : (0.99, 0.995)
##     No Information Rate : 0.284        
##     P-Value [Acc &gt; NIR] : &lt;2e-16       
##                                        
##                   Kappa : 0.991        
##  Mcnemar's Test P-Value : NA           
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity             0.999    0.986    0.992    0.983    1.000
## Specificity             0.997    0.999    0.996    0.999    1.000
## Pos Pred Value          0.992    0.995    0.983    0.995    1.000
## Neg Pred Value          1.000    0.997    0.998    0.997    1.000
## Prevalence              0.284    0.194    0.174    0.164    0.184
## Detection Rate          0.284    0.191    0.173    0.161    0.184
## Detection Prevalence    0.287    0.192    0.176    0.162    0.184
## Balanced Accuracy       0.998    0.993    0.994    0.991    1.000</code></pre>
</div>

<div>
<h1>
<a name="prediction-on-original-test-set" class="anchor" href="#prediction-on-original-test-set"><span class="octicon octicon-link"></span></a>Prediction on Original Test Set</h1>
<p>Finally we are ready to predict on the test set. We clean the test set using the same cleanup process we employed for the trainig set. In the test set the predicted variable is absent and is replaced by a problem_id column which is just the record #. Since this column was not in the training set we remove ot from the test set as well.</p>
<pre><code>## DATA CLEANUP AND PREPROCESSING : Testing set
## similar alogrithm as the training set above.
clean_test &lt;- test[,b &lt; 0.95]
clean_test &lt;- clean_test[,-c(1:7)]
clean_test &lt;- clean_test[,-which(colnames(clean_test)== "problem_id")]

## Predict on the testing set and list the predictions
clean_test_predict &lt;- predict(modFit,clean_test)
clean_test_predict</code></pre>
<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>
<p>This result was submitted to the programming assignment and got full points.</p>
</div>

<p></p>
</div>

<p>
</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
